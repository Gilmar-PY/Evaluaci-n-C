{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51cff10",
   "metadata": {},
   "source": [
    "### Respuestas al examen Parcial C8286"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33c7e0f",
   "metadata": {},
   "source": [
    "### Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f969de3",
   "metadata": {},
   "source": [
    "1.1 Para responder a esta pregunta, debemos ejecutar la simulación con las siguientes configuraciones:\n",
    "\n",
    "* Un solo trabajo (llamado 'a') con un tiempo de ejecución de 30 y un tamaño de conjunto de trabajo de 200.\n",
    "* Una CPU.\n",
    "* Activar la bandera -c para ver la respuesta final.\n",
    "* Activar la bandera -t para ver un rastro del trabajo paso a paso y cómo se programa.\n",
    "* El comando para ejecutar esta simulación sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810cae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "./multi.py -n 1 -L a:30:200 -c -t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d504bfb",
   "metadata": {},
   "source": [
    "Antes de ejecutar el comando, podemos analizar y predecir cuánto tiempo tomará completar el trabajo basándonos en la información proporcionada y las configuraciones de la simulación:\n",
    "\n",
    "- Tiempo de ejecución del trabajo: 30 unidades de tiempo.\n",
    "- Tamaño del conjunto de trabajo: 200.\n",
    "- Número de CPUs: 1.\n",
    "- Tamaño de la caché: 100 (por defecto).\n",
    "- Tiempo de calentamiento de la caché: 10 (por defecto).\n",
    "- Velocidad de ejecución en caché caliente: 2.\n",
    "- Velocidad de ejecución en caché fría: 1.\n",
    "\n",
    "Calentamiento de la caché:\n",
    "\n",
    "- El tamaño del conjunto de trabajo es 200, que es mayor que el tamaño de la caché (100).\n",
    "- Por lo tanto, el trabajo 'a' no cabe completamente en la caché.\n",
    "- Esto significa que el trabajo comenzará en caché fría.\n",
    "\n",
    "Ejecución del trabajo:\n",
    "\n",
    "- Al comenzar en caché fría, la velocidad de ejecución será 1.\n",
    "- El trabajo requiere un tiempo de ejecución de 30 unidades de tiempo.\n",
    "- Durante cada unidad de tiempo, el trabajo se ejecutará a la velocidad de 1 unidad de tiempo.\n",
    "\n",
    "Dado que la velocidad de ejecución en caché fría es 1 y el tiempo de ejecución del trabajo es 30, el tiempo total de finalización será 30 unidades de tiempo, ya que no se alcanza el tiempo necesario para calentar la caché completamente.\n",
    "\n",
    "Para confirmar esta predicción, ejecutemos el comando y observemos la salida:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edc05a8",
   "metadata": {},
   "source": [
    "Resultados esperados\n",
    "\n",
    "1. Rastro del trabajo paso a paso:\n",
    "\n",
    "* La bandera -t mostrará cómo el trabajo 'a' es programado en la única CPU.\n",
    "* Veremos que el trabajo se ejecuta continuamente en la CPU hasta que se complete su tiempo de ejecución de 30 unidades de tiempo.\n",
    "\n",
    "2. Respuesta final:\n",
    "\n",
    "* La bandera -c mostrará el tiempo total de finalización del trabajo, que esperamos que sea 30 unidades de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe705c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARG seed 0\n",
    "ARG job_num 3\n",
    "ARG max_run 100\n",
    "ARG max_wset 200\n",
    "ARG job_list a:30:200\n",
    "ARG affinity \n",
    "ARG per_cpu_queues False\n",
    "ARG num_cpus 1\n",
    "ARG quantum 10\n",
    "ARG peek_interval 30\n",
    "ARG warmup_time 10\n",
    "ARG cache_size 100\n",
    "ARG random_order False\n",
    "ARG trace True\n",
    "ARG trace_time False\n",
    "ARG trace_cache False\n",
    "ARG trace_sched False\n",
    "ARG compute True\n",
    "\n",
    "Nombre del trabajo:a tiempo_de_ejecución:30 tamaño_del_conjunto_de_trabajo:200\n",
    "\n",
    "Planificador cola central: ['a']\n",
    "\n",
    "   0   a      \n",
    "   1   a      \n",
    "   2   a      \n",
    "   3   a      \n",
    "   4   a      \n",
    "   5   a      \n",
    "   6   a      \n",
    "   7   a      \n",
    "   8   a      \n",
    "   9   a      \n",
    "----------\n",
    "  10   a      \n",
    "  11   a      \n",
    "  12   a      \n",
    "  13   a      \n",
    "  14   a      \n",
    "  15   a      \n",
    "  16   a      \n",
    "  17   a      \n",
    "  18   a      \n",
    "  19   a      \n",
    "----------\n",
    "  20   a      \n",
    "  21   a      \n",
    "  22   a      \n",
    "  23   a      \n",
    "  24   a      \n",
    "  25   a      \n",
    "  26   a      \n",
    "  27   a      \n",
    "  28   a      \n",
    "  29   a      \n",
    "\n",
    "Tiempo de finalización 30\n",
    "\n",
    "Estadísticas por CPU\n",
    "  CPU 0  utilización 100.00 [ caliente 0.00 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ec5f29",
   "metadata": {},
   "source": [
    "La primera simulación ejecutará el trabajo 'a' con un tiempo de ejecución de 30 y un tamaño de conjunto de trabajo de 200 en una sola CPU. Dado que el trabajo no cabe completamente en la caché y la velocidad de ejecución en caché fría es 1, tomará 30 unidades de tiempo completarse. Este análisis será confirmado al ejecutar el comando con las banderas -c y -t."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2684f55",
   "metadata": {},
   "source": [
    "1.2 Para responder a esta pregunta, necesitamos entender cómo el tamaño de la caché afecta la ejecución del trabajo y cómo los parámetros de calentamiento y la tasa de ejecución influyen en el rendimiento.\n",
    "\n",
    "Configuración del experimento\n",
    "\n",
    "* Trabajo 'a' con tiempo de ejecución de 30 y tamaño de conjunto de trabajo de 200.\n",
    "* Una CPU.\n",
    "* Tamaño de la caché: 300 (suficiente para contener el conjunto de trabajo de 200).\n",
    "* Tiempo de calentamiento de la caché: 10 (por defecto).\n",
    "* Tasa de ejecución en caché caliente: 2.\n",
    "* Tasa de ejecución en caché fría: 1.\n",
    "\n",
    "El comando para ejecutar esta simulación será:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "./multi.py -n 1 -L a:30:200 -M 300 -c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f39217",
   "metadata": {},
   "source": [
    "Dado que el tamaño de la caché es 300, que es mayor que el tamaño del conjunto de trabajo de 200, el conjunto de trabajo completo cabrá en la caché. Esto significa que después del tiempo de calentamiento, el trabajo se ejecutará a la tasa de ejecución en caché caliente.\n",
    "\n",
    "\n",
    "Calentamiento de la caché:\n",
    "\n",
    "- El tiempo de calentamiento de la caché es 10 unidades de tiempo.\n",
    "- Durante este tiempo, el trabajo se ejecutará a la tasa de ejecución en caché fría (1).\n",
    "\n",
    "Ejecución en caché caliente:\n",
    "\n",
    "- Después de 10 unidades de tiempo, la caché estará caliente.\n",
    "- La tasa de ejecución en caché caliente es 2.\n",
    "- El tiempo restante para completar el trabajo será 30 - 10 = 20 unidades de tiempo.\n",
    "- Con una tasa de ejecución de 2, el tiempo necesario para completar los 20 unidades de trabajo restantes será 20 / 2 = 10 unidades de tiempo.\n",
    "\n",
    "Tiempo total de ejecución:\n",
    "\n",
    "- Tiempo de calentamiento de la caché: 10 unidades de tiempo.\n",
    "- Tiempo de ejecución en caché caliente: 10 unidades de tiempo.\n",
    "- Tiempo total de ejecución predicho: 10 + 10 = 20 unidades de tiempo.\n",
    "\n",
    "Ejecutemos el comando para verificar nuestra predicción:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c395397",
   "metadata": {},
   "outputs": [],
   "source": [
    "./multi.py -n 1 -L a:30:200 -M 300 -c\n",
    "\n",
    "ARG seed 0\n",
    "ARG job_num 3\n",
    "ARG max_run 100\n",
    "ARG max_wset 200\n",
    "ARG job_list a:30:200\n",
    "ARG affinity \n",
    "ARG per_cpu_queues False\n",
    "ARG num_cpus 1\n",
    "ARG quantum 10\n",
    "ARG peek_interval 30\n",
    "ARG warmup_time 10\n",
    "ARG cache_size 300\n",
    "ARG random_order False\n",
    "ARG trace False\n",
    "ARG trace_time False\n",
    "ARG trace_cache False\n",
    "ARG trace_sched False\n",
    "ARG compute True\n",
    "\n",
    "Nombre del trabajo:a tiempo_de_ejecución:30 tamaño_del_conjunto_de_trabajo:200\n",
    "\n",
    "Planificador cola central: ['a']\n",
    "\n",
    "\n",
    "Tiempo de finalización 20\n",
    "\n",
    "Estadísticas por CPU\n",
    "  CPU 0  utilización 100.00 [ caliente 50.00 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc6e38",
   "metadata": {},
   "source": [
    "Respuesta final:\n",
    "\n",
    "* La bandera -c mostrará el tiempo total de finalización del trabajo.\n",
    "* Esperamos ver un tiempo total de finalización de 20 unidades de tiempo.\n",
    "\n",
    "Al ejecutar el comando, la salida debe confirmar que el tiempo total de ejecución del trabajo 'a' es 20 unidades de tiempo, una vez que el conjunto de trabajo cabe en la caché y se aprovecha la tasa de ejecución en caché caliente.\n",
    "\n",
    "Al aumentar el tamaño de la caché a 300, el conjunto de trabajo del trabajo 'a' cabe completamente en la caché. Después de un tiempo de calentamiento de 10 unidades de tiempo, el trabajo se ejecutará a una tasa de 2 unidades de tiempo por unidad de tiempo, completándose en 20 unidades de tiempo en total. Esta predicción se puede confirmar ejecutando el comando con la bandera de solución activada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2445a87d",
   "metadata": {},
   "source": [
    "1.3 Para ejecutar la simulación con el rastreo de tiempo restante habilitado, usaremos la bandera -T. Esto nos permitirá ver no solo qué trabajo se está programando en cada CPU en cada paso de tiempo, sino también el tiempo de ejecución que le queda a ese trabajo después de cada tick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "./multi.py -n 1 -L a:30:200 -M 300 -c -T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2bc6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "./multi.py -n 1 -L a:30:200 -M 300 -c -T\n",
    "ARG seed 0\n",
    "ARG job_num 3\n",
    "ARG max_run 100\n",
    "ARG max_wset 200\n",
    "ARG job_list a:30:200\n",
    "ARG affinity \n",
    "ARG per_cpu_queues False\n",
    "ARG num_cpus 1\n",
    "ARG quantum 10\n",
    "ARG peek_interval 30\n",
    "ARG warmup_time 10\n",
    "ARG cache_size 300\n",
    "ARG random_order False\n",
    "ARG trace False\n",
    "ARG trace_time True\n",
    "ARG trace_cache False\n",
    "ARG trace_sched False\n",
    "ARG compute True\n",
    "\n",
    "Nombre del trabajo:a tiempo_de_ejecución:30 tamaño_del_conjunto_de_trabajo:200\n",
    "\n",
    "Planificador cola central: ['a']\n",
    "\n",
    "   0   a [ 29]      \n",
    "   1   a [ 28]      \n",
    "   2   a [ 27]      \n",
    "   3   a [ 26]      \n",
    "   4   a [ 25]      \n",
    "   5   a [ 24]      \n",
    "   6   a [ 23]      \n",
    "   7   a [ 22]      \n",
    "   8   a [ 21]      \n",
    "   9   a [ 20]      \n",
    "----------------\n",
    "  10   a [ 18]      \n",
    "  11   a [ 16]      \n",
    "  12   a [ 14]      \n",
    "  13   a [ 12]      \n",
    "  14   a [ 10]      \n",
    "  15   a [  8]      \n",
    "  16   a [  6]      \n",
    "  17   a [  4]      \n",
    "  18   a [  2]      \n",
    "  19   a [  0]      \n",
    "\n",
    "Tiempo de finalización 20\n",
    "\n",
    "Estadísticas por CPU\n",
    "  CPU 0  utilización 100.00 [ caliente 50.00 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45567985",
   "metadata": {},
   "source": [
    "La salida confirma que el trabajo 'a' se completó en 20 unidades de tiempo, lo que es coherente con nuestra predicción cuando el conjunto de trabajo cabe en la caché.\n",
    "\n",
    "Ahora vamos a agregar más rastreo para mostrar el estado de cada caché de CPU para cada trabajo, utilizando la bandera -C. También analizaremos qué sucede cuando cambiamos el parámetro de tiempo de calentamiento (-w) a valores menores o mayores que el predeterminado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecee80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "./multi.py -n 1 -L a:30:200 -M 300 -c -T -C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f88f9",
   "metadata": {},
   "source": [
    "La ejecución confirma los detalles de cómo la caché se calienta y cómo se refleja en el tiempo de ejecución del trabajo. Aquí tienes un análisis detallado de la salida y lo que significa:\n",
    "\n",
    "Rastreo de caché y tiempo restante\n",
    "Durante los primeros 9 ticks:\n",
    "\n",
    "- La caché está fría para el trabajo 'a', indicada por espacios en blanco.\n",
    "- El tiempo restante del trabajo 'a' disminuye en 1 por cada tick, ya que se está ejecutando en caché fría.\n",
    "\n",
    "A partir del tick 10:\n",
    "\n",
    " - La caché se calienta para el trabajo 'a', indicada por 'w'.\n",
    "- El tiempo restante del trabajo 'a' disminuye en 2 por cada tick, ya que se está ejecutando en caché caliente.\n",
    "\n",
    "Tiempo de finalización y estadísticas\n",
    "\n",
    "- Tiempo de finalización: 20 unidades de tiempo.\n",
    "- Utilización de la CPU: 100%.\n",
    "- Calor en la CPU: 50%.\n",
    "\n",
    "Estos resultados confirman que, después de 10 unidades de tiempo (el tiempo de calentamiento predeterminado), la caché se calienta y la tasa de ejecución se incrementa, reduciendo el tiempo de ejecución restante más rápidamente.\n",
    "\n",
    "Ajuste del tiempo de calentamiento (-w)\n",
    "\n",
    "Ahora ajustemos el parámetro de tiempo de calentamiento (-w) a valores menores y mayores que el predeterminado para observar cómo cambia el comportamiento.\n",
    "\n",
    "Tiempo de calentamiento menor (-w 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05bfb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "./multi.py -n 1 -L a:30:200 -M 300 -c -T -C -w 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69e18fa",
   "metadata": {},
   "source": [
    "Predicción:\n",
    "\n",
    "- La caché se calentará después de 5 unidades de tiempo.\n",
    "- El trabajo 'a' se ejecutará en caché caliente después de 5 unidades de tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6456dca4",
   "metadata": {},
   "source": [
    "Tiempo de calentamiento mayor (-w 15)\n",
    "\n",
    "Ejecuta el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f093920",
   "metadata": {},
   "outputs": [],
   "source": [
    "./multi.py -n 1 -L a:30:200 -M 300 -c -T -C -w 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a2b48",
   "metadata": {},
   "source": [
    "Predicción:\n",
    "\n",
    "- La caché se calentará después de 15 unidades de tiempo.\n",
    "- El trabajo 'a' se ejecutará en caché caliente después de 15 unidades de tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec1495",
   "metadata": {},
   "source": [
    "El comportamiento del calentamiento de la caché y su impacto en el tiempo de ejecución del trabajo se observa claramente al ajustar el tiempo de calentamiento (-w). \n",
    "\n",
    "Tiempo de calentamiento predeterminado (-w 10):\n",
    "\n",
    "- La caché se calienta después de 10 unidades de tiempo.\n",
    "- El trabajo 'a' se completa en 20 unidades de tiempo.\n",
    "\n",
    "Tiempo de calentamiento menor (-w 5):\n",
    "\n",
    "- La caché se calienta más rápidamente.\n",
    "- El trabajo 'a' se ejecuta a una tasa más rápida más pronto, potencialmente reduciendo el tiempo total de ejecución.\n",
    "\n",
    "Tiempo de calentamiento mayor (-w 15):\n",
    "\n",
    " - La caché se calienta más lentamente.\n",
    "- El trabajo 'a' tarda más en beneficiarse de la tasa de ejecución más rápida, lo que puede aumentar el tiempo total de ejecución.\n",
    "\n",
    "Ajustar el tiempo de calentamiento de la caché es una herramienta útil para optimizar el rendimiento de los sistemas paralelos y distribuidos, dependiendo de las características específicas de la carga de trabajo y los recursos disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9868aa0",
   "metadata": {},
   "source": [
    "1.4 Para agregar más rastreo y mostrar el estado de cada caché de CPU para cada trabajo, utilizamos la bandera -C como hicimos en el ítem anterior. Esto nos permitirá ver si la caché está fría (espacio en blanco) o caliente (w) para cada trabajo. Además, analizaremos cómo cambia el comportamiento cuando ajustamos el parámetro de tiempo de calentamiento (-w)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063fd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "./multi.py -n 1 -L a:30:200 -M 300 -c -T -C "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5234d1",
   "metadata": {},
   "source": [
    "Compara la planificación centralizada y distribuida en términos de rendimiento y\n",
    "utilización de la CPU. Modifica el código para permitir la simulación con una cola centralizada y colas\n",
    "distribuidas.\n",
    "\n",
    "Posible soluciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a87c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulación con cola centralizada\n",
    "centralized_scheduler = Scheduler(\n",
    "    job_list='', affinity='', per_cpu_queues=False, peek_interval=30,\n",
    "    job_num=10, max_run=100, max_wset=200,\n",
    "    num_cpus=4, time_slice=10, random_order=False,\n",
    "    cache_size=100, cache_rate_cold=1, cache_rate_warm=2,\n",
    "    cache_warmup_time=10, solve=True,\n",
    "    trace=False, trace_time_left=False, trace_cache=False,\n",
    "    trace_sched=False\n",
    ")\n",
    "centralized_scheduler.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f3f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulación con colas distribuidas\n",
    "distributed_scheduler = Scheduler(\n",
    "    job_list='', affinity='', per_cpu_queues=True, peek_interval=30,\n",
    "    job_num=10, max_run=100, max_wset=200,\n",
    "    num_cpus=4, time_slice=10, random_order=False,\n",
    "    cache_size=100, cache_rate_cold=1, cache_rate_warm=2,\n",
    "    cache_warmup_time=10, solve=True,\n",
    "    trace=False, trace_time_left=False, trace_cache=False,\n",
    "    trace_sched=False\n",
    ")\n",
    "distributed_scheduler.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe752d",
   "metadata": {},
   "source": [
    "Después de ejecutar las simulaciones, reportaremos el tiempo total de ejecución y la utilización de la CPU para cada enfoque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de la simulación centralizada\n",
    "centralized_scheduler = Scheduler(\n",
    "    job_list='', affinity='', per_cpu_queues=False, peek_interval=30,\n",
    "    job_num=10, max_run=100, max_wset=200,\n",
    "    num_cpus=4, time_slice=10, random_order=False,\n",
    "    cache_size=100, cache_rate_cold=1, cache_rate_warm=2,\n",
    "    cache_warmup_time=10, solve=True,\n",
    "    trace=False, trace_time_left=True, trace_cache=False,\n",
    "    trace_sched=True\n",
    ")\n",
    "centralized_scheduler.run()\n",
    "\n",
    "print(\"\\n--- Simulación Centralizada ---\")\n",
    "print(f\"Tiempo de finalización: {centralized_scheduler.system_time}\")\n",
    "for cpu in range(centralized_scheduler.num_cpus):\n",
    "    print(f\"CPU {cpu} utilización: {100.0 * float(centralized_scheduler.stats_ran[cpu]) / float(centralized_scheduler.system_time):.2f}%\")\n",
    "\n",
    "# Configuración de la simulación distribuida\n",
    "distributed_scheduler = Scheduler(\n",
    "    job_list='', affinity='', per_cpu_queues=True, peek_interval=30,\n",
    "    job_num=10, max_run=100, max_wset=200,\n",
    "    num_cpus=4, time_slice=10, random_order=False,\n",
    "    cache_size=100, cache_rate_cold=1, cache_rate_warm=2,\n",
    "    cache_warmup_time=10, solve=True,\n",
    "    trace=False, trace_time_left=True, trace_cache=False,\n",
    "    trace_sched=True\n",
    ")\n",
    "distributed_scheduler.run()\n",
    "\n",
    "print(\"\\n--- Simulación Distribuida ---\")\n",
    "print(f\"Tiempo de finalización: {distributed_scheduler.system_time}\")\n",
    "for cpu in range(distributed_scheduler.num_cpus):\n",
    "    print(f\"CPU {cpu} utilización: {100.0 * float(distributed_scheduler.stats_ran[cpu]) / float(distributed_scheduler.system_time):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5759cd9",
   "metadata": {},
   "source": [
    "Recuerda: la planificación distribuida generalmente ofrece un mejor rendimiento y utilización de la CPU en sistemas con múltiples CPUs y trabajos concurrentes, ya que distribuye la carga de trabajo de manera más equilibrada. Sin embargo, es más compleja de implementar y gestionar en comparación con la planificación centralizada. Elegir entre estos enfoques depende de las características específicas del sistema y de las necesidades de rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c78a36",
   "metadata": {},
   "source": [
    "### Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da3c03",
   "metadata": {},
   "source": [
    "2.1  Añadir manejo de errores\n",
    "Para manejar errores de manera efectiva, vamos a envolver las secciones críticas del código en bloques try-except y asegurarnos de capturar y manejar cualquier excepción que pueda ocurrir.\n",
    "\n",
    "\n",
    "El módulo logging de Python nos permite registrar mensajes en diferentes niveles (info, warning, error, etc.). Configuraremos el logger para que registre los eventos importantes y errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec2ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import marshal\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import logging\n",
    "from queue import Empty, Queue\n",
    "import threading\n",
    "import types\n",
    "import chunk_mp_mapreduce as mr\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "work_queue = Queue()\n",
    "results_queue = Queue()\n",
    "results = {}\n",
    "\n",
    "async def submit_job(job_id, reader, writer):\n",
    "    try:\n",
    "        writer.write(job_id.to_bytes(4, 'little'))\n",
    "        await writer.drain()\n",
    "        code_size = int.from_bytes(await reader.read(4), 'little')\n",
    "        my_code = marshal.loads(await reader.read(code_size))\n",
    "        data_size = int.from_bytes(await reader.read(4), 'little')\n",
    "        data = pickle.loads(await reader.read(data_size))\n",
    "        work_queue.put_nowait((job_id, my_code, data))\n",
    "        logger.info(f\"Job {job_id} submitted successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error submitting job {job_id}: {e}\")\n",
    "        writer.write(b'\\x00\\x00\\x00\\x00')  # Indicar error al cliente\n",
    "        await writer.drain()\n",
    "\n",
    "def get_results_queue():\n",
    "    while results_queue.qsize() > 0:\n",
    "        try:\n",
    "            job_id, data = results_queue.get_nowait()\n",
    "            results[job_id] = data\n",
    "        except Empty:\n",
    "            return\n",
    "\n",
    "async def get_results(reader, writer):\n",
    "    try:\n",
    "        get_results_queue()\n",
    "        job_id = int.from_bytes(await reader.read(4), 'little')\n",
    "        data = pickle.dumps(None)\n",
    "        if job_id in results:\n",
    "            data = pickle.dumps(results[job_id])\n",
    "            del results[job_id]\n",
    "        writer.write(len(data).to_bytes(4, 'little'))\n",
    "        writer.write(data)\n",
    "        await writer.drain()\n",
    "        logger.info(f\"Results for job {job_id} retrieved successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving results: {e}\")\n",
    "        writer.write(b'\\x00\\x00\\x00\\x00')  # Indicar error al cliente\n",
    "        await writer.drain()\n",
    "\n",
    "async def accept_requests(reader, writer, job_id=[0]):\n",
    "    try:\n",
    "        op = await reader.read(1)\n",
    "        if op[0] == 0:\n",
    "            await submit_job(job_id[0], reader, writer)\n",
    "            job_id[0] += 1\n",
    "        elif op[0] == 1:\n",
    "            await get_results(reader, writer)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error accepting request: {e}\")\n",
    "\n",
    "def worker():\n",
    "    pool = mp.Pool()\n",
    "    while True:\n",
    "        try:\n",
    "            job_id, code, data = work_queue.get()  # blocking\n",
    "            func = types.FunctionType(code, globals(), 'mapper_and_reducer')\n",
    "            mapper, reducer = func()\n",
    "            counts = mr.map_reduce(pool, data, mapper, reducer, 100, mr.reporter)\n",
    "            results_queue.put((job_id, counts))\n",
    "            logger.info(f\"Trabajo {job_id} procesado exitosamente .\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error en el procesamiento de trabajo del worker {job_id}: {e}\")\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "async def main():\n",
    "    try:\n",
    "        server = await asyncio.start_server(accept_requests, '127.0.0.1', 1936)\n",
    "        worker_thread = threading.Thread(target=worker, daemon=True)\n",
    "        worker_thread.start()\n",
    "        async with server:\n",
    "            logger.info(\"Servidor inicializado exitosamente.\")\n",
    "            await server.serve_forever()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inicializando el servidor : {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        asyncio.run(main())\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error fatal: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3943b",
   "metadata": {},
   "source": [
    "Explicación de los cambios:\n",
    "\n",
    "- Se configuró el módulo logging con un formato básico y un nivel de logging de INFO.\n",
    "- Cada función relevante (submit_job, get_results, accept_requests, worker, main) ahora tiene bloques try-except para capturar y manejar excepciones.\n",
    "- En caso de error, se registra un mensaje de error usando logger.error y se envía una respuesta al cliente si es necesario.\n",
    "- await writer.drain() se añadió después de writer.write() para asegurarse de que los datos se envíen completamente al cliente.\n",
    "- Se añadieron mensajes de información (logger.info) para indicar el éxito de varias operaciones (por ejemplo, trabajo enviado, resultados recuperados, servidor iniciado).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb90c0",
   "metadata": {},
   "source": [
    "2.2 Vamos a refactorizar la función worker para utilizar concurrent.futures.ProcessPoolExecutor en lugar de multiprocessing.Pool. Esto nos permitirá manejar los procesos de manera más eficiente y moderna.\n",
    "\n",
    "Primero, refactorizaremos la función worker y luego ajustaremos el código para utilizar ProcessPoolExecutor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3952047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import marshal\n",
    "import pickle\n",
    "import logging\n",
    "from queue import Empty, Queue\n",
    "import threading\n",
    "import types\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import chunk_mp_mapreduce as mr\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "work_queue = Queue()\n",
    "results_queue = Queue()\n",
    "results = {}\n",
    "\n",
    "async def submit_job(job_id, reader, writer):\n",
    "    try:\n",
    "        writer.write(job_id.to_bytes(4, 'little'))\n",
    "        await writer.drain()\n",
    "        code_size = int.from_bytes(await reader.read(4), 'little')\n",
    "        my_code = marshal.loads(await reader.read(code_size))\n",
    "        data_size = int.from_bytes(await reader.read(4), 'little')\n",
    "        data = pickle.loads(await reader.read(data_size))\n",
    "        work_queue.put_nowait((job_id, my_code, data))\n",
    "        logger.info(f\"Trabajo {job_id} entregado exitosamente.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en entregar el trabajo {job_id}: {e}\")\n",
    "        writer.write(b'\\x00\\x00\\x00\\x00')  # Indicar error al cliente\n",
    "        await writer.drain()\n",
    "\n",
    "def get_results_queue():\n",
    "    while results_queue.qsize() > 0:\n",
    "        try:\n",
    "            job_id, data = results_queue.get_nowait()\n",
    "            results[job_id] = data\n",
    "        except Empty:\n",
    "            return\n",
    "\n",
    "async def get_results(reader, writer):\n",
    "    try:\n",
    "        get_results_queue()\n",
    "        job_id = int.from_bytes(await reader.read(4), 'little')\n",
    "        data = pickle.dumps(None)\n",
    "        if job_id in results:\n",
    "            data = pickle.dumps(results[job_id])\n",
    "            del results[job_id]\n",
    "        writer.write(len(data).to_bytes(4, 'little'))\n",
    "        writer.write(data)\n",
    "        await writer.drain()\n",
    "        logger.info(f\"Resultado para el trabajo {job_id} recuperado exitosamente.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en el resultado recuperado: {e}\")\n",
    "        writer.write(b'\\x00\\x00\\x00\\x00')  # Indicar error al cliente\n",
    "        await writer.drain()\n",
    "\n",
    "async def accept_requests(reader, writer, job_id=[0]):\n",
    "    try:\n",
    "        op = await reader.read(1)\n",
    "        if op[0] == 0:\n",
    "            await submit_job(job_id[0], reader, writer)\n",
    "            job_id[0] += 1\n",
    "        elif op[0] == 1:\n",
    "            await get_results(reader, writer)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al aceptar la solicitud: {e}\")\n",
    "\n",
    "def worker():\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        while True:\n",
    "            try:\n",
    "                job_id, code, data = work_queue.get()  # blocking\n",
    "                func = types.FunctionType(code, globals(), 'mapper_and_reducer')\n",
    "                mapper, reducer = func()\n",
    "                future = executor.submit(mr.map_reduce, data, mapper, reducer, 100, mr.reporter)\n",
    "                future.add_done_callback(lambda f, job_id=job_id: results_queue.put((job_id, f.result())))\n",
    "                logger.info(f\"Trabajo {job_id} entregado al ejeecutor.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error en el procesamiento en el trabajo del worker {job_id}: {e}\")\n",
    "\n",
    "async def main():\n",
    "    try:\n",
    "        server = await asyncio.start_server(accept_requests, '127.0.0.1', 1936)\n",
    "        worker_thread = threading.Thread(target=worker, daemon=True)\n",
    "        worker_thread.start()\n",
    "        async with server:\n",
    "            logger.info(\"El servidor se inicia correctamente.\")\n",
    "            await server.serve_forever()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al iniciar el servidor: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        asyncio.run(main())\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error fatal: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e066b",
   "metadata": {},
   "source": [
    "Explicación de los cambios\n",
    "\n",
    "- La función worker ahora utiliza concurrent.futures.ProcessPoolExecutor.\n",
    "- executor.submit se usa para enviar tareas al pool de procesos.\n",
    "- future.add_done_callback se usa para agregar una función de callback que coloca los resultados en results_queue una vez que la tarea se completa.\n",
    "- Manejamos excepciones dentro del bucle while en worker para asegurarnos de que los errores no detengan el procesamiento de trabajos futuros.\n",
    "- Utilizamos with ProcessPoolExecutor() as executor: para asegurar que el executor se cierre correctamente una vez que el programa termina.\n",
    "\n",
    "Este enfoque utiliza concurrent.futures.ProcessPoolExecutor para mejorar la gestión de procesos y hace que el código sea más limpio y moderno como vimos en clase. Además, con el uso de callbacks, podemos manejar los resultados de manera más eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddf1f38",
   "metadata": {},
   "source": [
    "2.3 Vamos a optimizar la comunicación asíncrona utilizando asyncio.Queue en lugar de queue.Queue para work_queue y results_queue, asegurándonos de que todas las operaciones de cola sean asíncronas. Esto mejorará la eficiencia y permitirá un mejor manejo de las tareas en un entorno asíncrono.\n",
    "\n",
    "Vamos a refactorizar el código:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f283cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import marshal\n",
    "import pickle\n",
    "import logging\n",
    "import threading\n",
    "import types\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import chunk_mp_mapreduce as mr\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "work_queue = asyncio.Queue()\n",
    "results_queue = asyncio.Queue()\n",
    "results = {}\n",
    "\n",
    "async def submit_job(job_id, reader, writer):\n",
    "    try:\n",
    "        writer.write(job_id.to_bytes(4, 'little'))\n",
    "        await writer.drain()\n",
    "        code_size = int.from_bytes(await reader.read(4), 'little')\n",
    "        my_code = marshal.loads(await reader.read(code_size))\n",
    "        data_size = int.from_bytes(await reader.read(4), 'little')\n",
    "        data = pickle.loads(await reader.read(data_size))\n",
    "        await work_queue.put((job_id, my_code, data))\n",
    "        logger.info(f\"Trabajo {job_id} entregado exitosamente.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en entregar el trabajo {job_id}: {e}\")\n",
    "        \n",
    "        writer.write(b'\\x00\\x00\\x00\\x00')  # Indicar error al cliente\n",
    "        await writer.drain()\n",
    "\n",
    "async def get_results_queue():\n",
    "    while not results_queue.empty():\n",
    "        try:\n",
    "            job_id, data = await results_queue.get()\n",
    "            results[job_id] = data\n",
    "        except asyncio.QueueEmpty:\n",
    "            return\n",
    "\n",
    "async def get_results(reader, writer):\n",
    "    try:\n",
    "        await get_results_queue()\n",
    "        job_id = int.from_bytes(await reader.read(4), 'little')\n",
    "        data = pickle.dumps(None)\n",
    "        if job_id in results:\n",
    "            data = pickle.dumps(results[job_id])\n",
    "            del results[job_id]\n",
    "        writer.write(len(data).to_bytes(4, 'little'))\n",
    "        writer.write(data)\n",
    "        await writer.drain()\n",
    "        logger.info(f\"Resultados para trabajos {job_id} recuperados exitosamente.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en los resultados esperados: {e}\")\n",
    "        writer.write(b'\\x00\\x00\\x00\\x00')  # Indicar error al cliente\n",
    "        await writer.drain()\n",
    "\n",
    "async def accept_requests(reader, writer, job_id=[0]):\n",
    "    try:\n",
    "        op = await reader.read(1)\n",
    "        if op[0] == 0:\n",
    "            await submit_job(job_id[0], reader, writer)\n",
    "            job_id[0] += 1\n",
    "        elif op[0] == 1:\n",
    "            await get_results(reader, writer)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al aceptar la solicitud: {e}\")\n",
    "\n",
    "async def worker():\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        while True:\n",
    "            try:\n",
    "                job_id, code, data = await work_queue.get()\n",
    "                func = types.FunctionType(code, globals(), 'mapper_and_reducer')\n",
    "                mapper, reducer = func()\n",
    "                loop = asyncio.get_event_loop()\n",
    "                future = loop.run_in_executor(executor, mr.map_reduce, data, mapper, reducer, 100, mr.reporter)\n",
    "                result = await future\n",
    "                await results_queue.put((job_id, result))\n",
    "                logger.info(f\"Trabajo {job_id} procesados exitosamente.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error en el procesamiento de trabajo del worker{job_id}: {e}\")\n",
    "\n",
    "async def main():\n",
    "    try:\n",
    "        server = await asyncio.start_server(accept_requests, '127.0.0.1', 1936)\n",
    "        worker_task = asyncio.create_task(worker())\n",
    "        async with server:\n",
    "            logger.info(\"Servidor iniciado exitosamente.\")\n",
    "            await server.serve_forever()\n",
    "        await worker_task\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error de inicio del servidor: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        asyncio.run(main())\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error fatal: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1cd242",
   "metadata": {},
   "source": [
    "Explicación de los cambios\n",
    "\n",
    "\n",
    "- Cambiamos Queue por asyncio.Queue para work_queue y results_queue.\n",
    "- Utilizamos await work_queue.put() y await results_queue.get() para operaciones de encolado y desencolado de manera asíncrona.\n",
    "- En get_results_queue, usamos await results_queue.get() para manejar la cola de resultados de manera asíncrona.\n",
    "- En worker, utilizamos asyncio.get_event_loop().run_in_executor() para ejecutar mr.map_reduce en el ProcessPoolExecutor de manera asíncrona.\n",
    "- Mantuvimos el manejo de errores existente para capturar y registrar cualquier excepción que ocurra durante el procesamiento de trabajos y la comunicación.\n",
    "- Creamos una tarea asíncrona para el trabajador con asyncio.create_task(worker()) en lugar de un hilo separado.\n",
    "\n",
    "Estos cambios optimizan la comunicación asíncrona y aseguran que las operaciones de cola se manejen de manera eficiente en un entorno asíncrono."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66678adf",
   "metadata": {},
   "source": [
    "2.4 Vamos a contenerizar la aplicación usando Docker. El objetivo es crear un Dockerfile que nos permita construir una imagen de Docker que ejecute la aplicación. Luego, probaremos esta imagen para verificar que la aplicación funciona correctamente dentro del contenedor.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78dcac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar una imagen base de Python\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Establecer el directorio de trabajo\n",
    "WORKDIR /app\n",
    "\n",
    "# Copiar los archivos requeridos al contenedor\n",
    "COPY requirements.txt requirements.txt\n",
    "COPY chunk_mp_mapreduce.py chunk_mp_mapreduce.py\n",
    "COPY main.py main.py\n",
    "\n",
    "# Instalar las dependencias\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Exponer el puerto en el que la aplicación va a correr\n",
    "EXPOSE 1936\n",
    "\n",
    "# Comando para ejecutar la aplicación\n",
    "CMD [\"python\", \"main.py\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49da8bb",
   "metadata": {},
   "source": [
    "Creamos el archivo requirements.txt. Este archivo contendrá las dependencias de Python necesarias para nuestra aplicación.\n",
    "\n",
    "requirements.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80591833",
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b0470",
   "metadata": {},
   "source": [
    "Guardamos el código de la aplicación en un archivo llamado main.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import marshal\n",
    "import pickle\n",
    "import logging\n",
    "import threading\n",
    "import types\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import chunk_mp_mapreduce as mr\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "work_queue = asyncio.Queue()\n",
    "results_queue = asyncio.Queue()\n",
    "results = {}\n",
    "\n",
    "async def submit_job(job_id, reader, writer):\n",
    "    try:\n",
    "        writer.write(job_id.to_bytes(4, 'little'))\n",
    "        await writer.drain()\n",
    "        code_size = int.from_bytes(await reader.read(4), 'little')\n",
    "        my_code = marshal.loads(await reader.read(code_size))\n",
    "        data_size = int.from_bytes(await reader.read(4), 'little')\n",
    "        data = pickle.loads(await reader.read(data_size))\n",
    "        await work_queue.put((job_id, my_code, data))\n",
    "        logger.info(f\" ... {job_id} ...\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"... {job_id}: {e}\")\n",
    "        writer.write(b'\\x00\\x00\\x00\\x00')  # Indicar error al cliente\n",
    "        await writer.drain()\n",
    "\n",
    "async def get_results_queue():\n",
    "    while not results_queue.empty():\n",
    "        try:\n",
    "            job_id, data = await results_queue.get()\n",
    "            results[job_id] = data\n",
    "        except asyncio.QueueEmpty:\n",
    "            return\n",
    "\n",
    "async def get_results(reader, writer):\n",
    "    try:\n",
    "        await get_results_queue()\n",
    "        job_id = int.from_bytes(await reader.read(4), 'little')\n",
    "        data = pickle.dumps(None)\n",
    "        if job_id in results:\n",
    "            data = pickle.dumps(results[job_id])\n",
    "            del results[job_id]\n",
    "        writer.write(len(data).to_bytes(4, 'little'))\n",
    "        writer.write(data)\n",
    "        await writer.drain()\n",
    "        logger.info(f\"Resultados ...{job_id} ...\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"...: {e}\")\n",
    "        writer.write(b'\\x00\\x00\\x00\\x00')  # Indicar error al cliente\n",
    "        await writer.drain()\n",
    "\n",
    "async def accept_requests(reader, writer, job_id=[0]):\n",
    "    try:\n",
    "        op = await reader.read(1)\n",
    "        if op[0] == 0:\n",
    "            await submit_job(job_id[0], reader, writer)\n",
    "            job_id[0] += 1\n",
    "        elif op[0] == 1:\n",
    "            await get_results(reader, writer)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"...: {e}\")\n",
    "\n",
    "async def worker():\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        while True:\n",
    "            try:\n",
    "                job_id, code, data = await work_queue.get()\n",
    "                func = types.FunctionType(code, globals(), 'mapper_and_reducer')\n",
    "                mapper, reducer = func()\n",
    "                loop = asyncio.get_event_loop()\n",
    "                future = loop.run_in_executor(executor, mr.map_reduce, data, mapper, reducer, 100, mr.reporter)\n",
    "                result = await future\n",
    "                await results_queue.put((job_id, result))\n",
    "                logger.info(f\"... {job_id} ....\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"... {job_id}: {e}\")\n",
    "\n",
    "async def main():\n",
    "    try:\n",
    "        server = await asyncio.start_server(accept_requests, '0.0.0.0', 1936)\n",
    "        worker_task = asyncio.create_task(worker())\n",
    "        async with server:\n",
    "            logger.info(\".....\")\n",
    "            await server.serve_forever()\n",
    "        await worker_task\n",
    "    except Exception as e:\n",
    "        logger.error(f\"...: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        asyncio.run(main())\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error fatal: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1098df2",
   "metadata": {},
   "source": [
    "Abrimos una terminal y ejecutamos el siguiente comando para construir la imagen de Docker:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d94c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker build -t mapreduce_app ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359588d0",
   "metadata": {},
   "source": [
    "Después de construir la imagen, podemos ejecutar un contenedor basado en esta imagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854eedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -p 1936:1936 mapreduce_app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39f53f9",
   "metadata": {},
   "source": [
    "Para verificar que la aplicación funciona correctamente dentro del contenedor, podemos conectar un cliente al puerto 1936 en el host y enviar trabajos de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a512c3",
   "metadata": {},
   "source": [
    "2.5 Vamos a desplegar la aplicación en un clúster de Kubernetes. Esto implica crear los archivos de configuración necesarios (Deployment y Service) para Kubernetes. Estos archivos nos permitirán gestionar la aplicación y asegurar que se escale y administre correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c56ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "### deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: mapreduce-app\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: mapreduce-app\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: mapreduce-app\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: mapreduce-app\n",
    "        image: mapreduce_app:latest\n",
    "        ports:\n",
    "        - containerPort: 1936\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"64Mi\"\n",
    "            cpu: \"250m\"\n",
    "          limits:\n",
    "            memory: \"128Mi\"\n",
    "            cpu: \"500m\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670e655",
   "metadata": {},
   "source": [
    "El archivo de Service define cómo se expone la aplicación a través de un puerto accesible externamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b83e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "### services.yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: mapreduce-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: mapreduce-app\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 1936\n",
    "      targetPort: 1936\n",
    "  type: LoadBalancer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d494776",
   "metadata": {},
   "source": [
    "Para que Kubernetes pueda acceder a la imagen de Docker, debemos subirla a un registro de imágenes (como Docker Hub o Google Container Registry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0715827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar sesión en Docker Hub\n",
    "docker login\n",
    "\n",
    "# Etiquetar la imagen\n",
    "docker tag mapreduce_app:latest <your-dockerhub-username>/mapreduce_app:latest\n",
    "\n",
    "# Subir la imagen a Docker Hub\n",
    "docker push <your-dockerhub-username>/mapreduce_app:latest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f35d3d",
   "metadata": {},
   "source": [
    "Actualizamos el archivo deployment.yaml para usar la imagen del registro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7071121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "containers:\n",
    "- name: mapreduce-app\n",
    "  image: <your-dockerhub-username>/mapreduce_app:latest\n",
    "  ports:\n",
    "  - containerPort: 1936\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a3d44f",
   "metadata": {},
   "source": [
    "Usamos kubectl para aplicar los archivos de configuración y desplegar la aplicación en el clúster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f deployment.yaml\n",
    "kubectl apply -f service.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e7ad2e",
   "metadata": {},
   "source": [
    "Podemos verificar que los recursos se hayan creado correctamente y que la aplicación esté corriendo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e92a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar el estado de los pods\n",
    "kubectl get pods\n",
    "\n",
    "# Verificar el estado del servicio\n",
    "kubectl get services\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687fea8",
   "metadata": {},
   "source": [
    "2.6 Todo lo pedido se resumen aquí:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ec489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import itertools\n",
    "import collections\n",
    "from typing import Callable, Any, List, Dict, Tuple\n",
    "\n",
    "def chunk_data(data: List[Any], chunk_size: int) -> List[List[Any]]:\n",
    "    \"\"\"Divide los datos en trozos más pequeños.\"\"\"\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        yield data[i:i + chunk_size]\n",
    "\n",
    "def map_reduce(\n",
    "        data: List[Any],\n",
    "        mapper: Callable[[Any], List[Tuple[Any, Any]]],\n",
    "        reducer: Callable[[Any, List[Any]], Any],\n",
    "        chunk_size: int,\n",
    "        reporter: Callable[[str], None] = None,\n",
    "        combiner: Callable[[Any, List[Any]], List[Tuple[Any, Any]]] = None,\n",
    "        partitioner: Callable[[Any], int] = None,\n",
    "        num_reducers: int = 4) -> Dict[Any, Any]:\n",
    "    \n",
    "    if reporter:\n",
    "        reporter(\"Inicio de la fase map \")\n",
    "    \n",
    "    # Fase de Mapeo\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        chunks = list(chunk_data(data, chunk_size))\n",
    "        map_results = list(executor.map(mapper, chunks))\n",
    "\n",
    "    if reporter:\n",
    "        reporter(\"Fase map completada\")\n",
    "\n",
    "    if reporter:\n",
    "        reporter(\"Inicio de la fase combine\")\n",
    "    \n",
    "    # Fase de Combinación (opcional)\n",
    "    if combiner:\n",
    "        combined_results = []\n",
    "        for chunk in map_results:\n",
    "            combined_chunk = collections.defaultdict(list)\n",
    "            for key, value in chunk:\n",
    "                combined_chunk[key].append(value)\n",
    "            combined_results.append(\n",
    "                list(itertools.chain.from_iterable(combiner(k, v) for k, v in combined_chunk.items()))\n",
    "            )\n",
    "        map_results = combined_results\n",
    "\n",
    "    if reporter:\n",
    "        reporter(\" Fase de combine completada\")\n",
    "\n",
    "    if reporter:\n",
    "        reporter(\"Inicio de la fase shuffle y sort\")\n",
    "    \n",
    "    # Fase de Barajado y Ordenación\n",
    "    shuffle_sort = collections.defaultdict(list)\n",
    "    for result in map_results:\n",
    "        for key, value in result:\n",
    "            shuffle_sort[key].append(value)\n",
    "\n",
    "    if partitioner:\n",
    "        partitioned_data = collections.defaultdict(list)\n",
    "        for key, values in shuffle_sort.items():\n",
    "            partitioned_data[partitioner(key) % num_reducers].append((key, values))\n",
    "    else:\n",
    "        partitioned_data = {i: [] for i in range(num_reducers)}\n",
    "        for i, (key, values) in enumerate(shuffle_sort.items()):\n",
    "            partitioned_data[i % num_reducers].append((key, values))\n",
    "\n",
    "    if reporter:\n",
    "        reporter(\" Fase shuffle y sort completada\")\n",
    "\n",
    "    if reporter:\n",
    "        reporter(\"Inicio de la fase reduce\")\n",
    "    \n",
    "    # Fase de Reducción\n",
    "    reduce_results = {}\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for partition, kv_pairs in partitioned_data.items():\n",
    "            futures.append(executor.submit(reduce_partition, kv_pairs, reducer))\n",
    "        for future in futures:\n",
    "            reduce_results.update(future.result())\n",
    "\n",
    "    if reporter:\n",
    "        reporter(\"Fase reduce completada\")\n",
    "    \n",
    "    return reduce_results\n",
    "\n",
    "def reduce_partition(kv_pairs: List[Tuple[Any, List[Any]]], reducer: Callable[[Any, List[Any]], Any]) -> Dict[Any, Any]:\n",
    "    \"\"\"Reduce una partición de datos.\"\"\"\n",
    "    reduced_data = {}\n",
    "    for key, values in kv_pairs:\n",
    "        reduced_data[key] = reducer(key, values)\n",
    "    return reduced_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68071106",
   "metadata": {},
   "source": [
    "2.7 Posible soluciones\n",
    "\n",
    "- Implementar un manejador de señales para limpiar recursos y cerrar el servidor de manera ordenada.\n",
    "- Utilizar functools.partial para simplificar la creación de funciones con múltiples parámetros predefinidos.\n",
    "- Utilizar mp.Pool con un inicializador que configure la gestión de señales apropiadamente en los procesos hijos.\n",
    "- Enviar un trabajo especial (sentinela) a los trabajadores para indicarles que deben finaliza y asegurarse de que los hilos trabajadores se unen adecuadamente antes de que el programa principal termine.\n",
    "- Capturar y manejar las excepciones en las funciones asincrónicas y en los hilos trabajadoresy utilizar bloques try-except para manejar errores en la comunicación entre el cliente y el servidor.\n",
    "- Refactorizar el código para separar claramente las responsabilidades (por ejemplo, manejar trabajos, manejar resultados, aceptar solicitudes) y utilizar nombres de variables y funciones descriptivos para mejorar la claridad del código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a4c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
